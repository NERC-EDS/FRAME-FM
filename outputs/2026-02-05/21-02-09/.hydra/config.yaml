data:
  _target_: FRAME_FM.dataloaders.demo_geotiff_dataloader.XarrayStaticDataModule
  data_root: /gws/ssde/j25b/eds_ai/frame-fm/data/inputs/land_cover_map_2015/data/LCM2015_GB_1km_percent_cover_aggregate_class.tif
  batch_size: 8
  num_workers: 4
  pin_memory: true
  persistent_workers: true
  split_strategy: fraction
  train_split: 0.85
  val_split: 0.15
  test_split: 0.0
  train_indices: null
  val_indices: null
  test_indices: null
  tile_size: 64
  train_transforms:
    _target_: torchvision.transforms.Compose
    transforms:
    - _target_: torchvision.transforms.RandomHorizontalFlip
      p: 0.5
    - _target_: torchvision.transforms.Normalize
      mean:
      - 0
      - 0
      - 0
      - 0
      - 0
      - 0
      - 0
      - 0
      - 0
      - 0
      std:
      - 100
      - 100
      - 100
      - 100
      - 100
      - 100
      - 100
      - 100
      - 100
      - 100
    - _target_: torchvision.transforms.Normalize
      mean:
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      std:
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
  val_transforms:
    _target_: torchvision.transforms.Compose
    transforms:
    - _target_: torchvision.transforms.Normalize
      mean:
      - 0
      - 0
      - 0
      - 0
      - 0
      - 0
      - 0
      - 0
      - 0
      - 0
      std:
      - 100
      - 100
      - 100
      - 100
      - 100
      - 100
      - 100
      - 100
      - 100
      - 100
    - _target_: torchvision.transforms.Normalize
      mean:
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      std:
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
  test_transforms:
    _target_: torchvision.transforms.Compose
    transforms:
    - _target_: torchvision.transforms.Normalize
      mean:
      - 0
      - 0
      - 0
      - 0
      - 0
      - 0
      - 0
      - 0
      - 0
      - 0
      std:
      - 100
      - 100
      - 100
      - 100
      - 100
      - 100
      - 100
      - 100
      - 100
      - 100
    - _target_: torchvision.transforms.Normalize
      mean:
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      std:
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
      - 0.5
model:
  _target_: FRAME_FM.models.demo_convAE.ConvAutoencoder
  in_channels: 10
  base_channels: 32
  kernel_size: 3
  latent_dim: 256
  lr: 0.001
  weight_decay: 1.0e-05
  num_classes: 17
trainer:
  _target_: pytorch_lightning.Trainer
  max_epochs: 50
  accelerator: auto
  devices: auto
  precision: 32
  log_every_n_steps: 5
  enable_checkpointing: true
  enable_progress_bar: true
  enable_model_summary: true
logging:
  _target_: FRAME_FM.training.logger.create_mlflow_logger
  experiment_name: frame-fm-land-cover-convae
  tracking_uri: ${oc.env:MLFLOW_TRACKING_URI, "file:./mlruns"}
  run_name: land_cover_convae
  tags:
    project: FRAME-FM
    dataset: Land Cover Map
    model: Land Cover Convolutional Autoencoder
seed: 42
